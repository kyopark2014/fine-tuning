{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e902c6d1-9636-451c-9205-78fa5d13bcc6",
   "metadata": {},
   "source": [
    "### Reference: [Korean LLM (Large Language Model) fine-tuning on Local environment (Debugging)](https://github.com/daekeun-ml/genai-ko-LLM/blob/main/fine-tuning/2_local-train-debug-lora.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd4390-5d0d-40a9-854e-118925bf79c9",
   "metadata": {},
   "source": [
    "허깅페이스 인증 정보 설정: huggingface-cli login\n",
    "\n",
    "https://huggingface.co/join\n",
    "\n",
    "https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f771f3-4d1d-4290-804b-ccad7dac8a31",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "본격적으로 SageMaker 훈련 인스턴스로 훈련을 수행하기 전에 SageMaker Notebook / SageMaker Studio / SageMaker Studio Lab 에서 샘플 데이터로 디버깅을 수행합니다. 물론 온프레미스 환경에서 디버깅을 수행할 수 있다면, 기존 환경과 동일하게 디버깅을 수행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9153a0b-f312-45f6-bc50-e8e532b6b065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de57f668-b454-4ff4-9fd2-7fc102a289a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r bucket_prefix dataset_prefix s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1917138-0c73-4e0c-ae78-bbcc90a009c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko-llms/peft'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169c1f3f-4079-464e-80a8-6f29d2a3b588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunk-train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cb2669-4f43-4198-8129-98ae9168fa91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-677146750822/ko-llms/peft/kullm-polyglot-12-8b-v2/dataset/chunk-train'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152ed89a-4764-4d74-a820-5c1ec6339634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_prefix\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] 1번 모듈 노트북을 다시 실행해 주세요.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab071132-6227-4477-8830-501ec50ff8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset = load_from_disk(dataset_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9ca6f-bfa3-4d7f-8c5a-eab50146d707",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "한정된 GPU 메모리 안에 LLM 모델을 로드하는 것은 매우 어렵습니다. 예컨대 20B의 모델을 로드하려면 fp32 기준으로 80GB 이상의 메모리가 필요하고 fp16 기준으로도 40GB 이상의 GPU 메모리가 필요하며, 파인 튜닝을 수행하는 경우는 이보다 더욱 많은 GPU 메모리가 필요합니다. 이런 경우 4비트 양자화와 LoRA를 사용하면 범용적으로 사용하고 있는 16GB 및 24GB GPU 메모리로도 파인 튜닝이 가능합니다. 현 기준으로는 4비트 양자화를 지원하는 QLoRA 기법이 가장 널리 사용되고 있으며 bitsandbytes를 사용하여 QLoRA를 쉽게 적용할 수 있습니다. QLoRA는 양자화된 파라미터의 분포 범위를 정규 분포 내로 억제하여 정밀도의 저하를 방지하는 4비트 NormalFloat 양자화 양자화를 적용하는 정수에 대해서도 양자화를 적용하는 이중 양자화, 그리고 optimizer state 등의 데이터를 CPU 메모리에 저장하는 페이징 기법을 적용하여 GPU 메모리 사용량을 억제합니다. QLoRA에 대한 자세한 내용은 논문 (https://arxiv.org/pdf/2305.14314.pdf) 을 참조하기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1c655-89b6-43fc-b40e-2834719b0fde",
   "metadata": {},
   "source": [
    "## Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9a18212-910a-4027-8346-1cc41aee36f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quant_4bit = True\n",
    "quant_8bit = False\n",
    "\n",
    "if quant_4bit:\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "else:\n",
    "    nf4_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a78b1b-3590-470f-8e47-95b6391db689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf4_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9359a9-5521-409b-af3e-bf1b677c25f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddea92c-3457-4682-8fda-c83c06c5f7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4e516f-6d48-42cb-9e9b-8dbec8b976af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPTNeoXTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(HF_MODEL_ID)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoXForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_tar_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquant_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#cache_dir=cache_dir,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnf4_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/modeling_utils.py:2897\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m-> 2897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m   2899\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2900\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2901\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install bitsandbytes`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_tar_dir,\n",
    "    load_in_8bit=True if quant_8bit else False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    #cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4587035-4d75-4e57-bd80-f4fa14c4a09e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create LoRA config\n",
    "\n",
    "LoRA 설정에 대한 자세한 내용은 아래를 참조해 주세요.\n",
    "\n",
    "https://huggingface.co/docs/peft/conceptual_guides/lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210f081-3dd6-47e5-a1b9-53d802c1007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_r  = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"query_key_value\", \"xxx\"]\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b35ccb-b4f5-4eb9-ba44-bfc13e29d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abca924-7205-4d70-8065-1d1c8fd6952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac231cc1-1b9e-43e8-87ee-cbc76bc53454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lm_dataset\n",
    "val_data = None\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 3e-5\n",
    "gradient_accumulation_steps = 2\n",
    "val_set_size = 0\n",
    "output_dir = 'output'\n",
    "world_size = 1\n",
    "ddp = world_size != 1\n",
    "group_by_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52de318-bc7b-4bcb-b71a-4bb280064359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200 if val_set_size > 0 else None,\n",
    "    save_steps=10,\n",
    "    output_dir=output_dir,\n",
    "    load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "    ddp_find_unused_parameters=False if ddp else None,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=group_by_length,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629160ad-9db2-4db2-9fb9-3316c23311b9",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13022221-6931-4344-b628-6a1895df9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "#     model, type(model)\n",
    "# )\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d670259-7d03-4f5c-9b23-21e083561021",
   "metadata": {},
   "source": [
    "## Check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61cb5f-0ac9-414d-a9fc-954f127250a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "#trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508589e3-9cdb-4eea-8f04-f116602cfb0f",
   "metadata": {},
   "source": [
    "## Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e927c9-3b38-4ca0-95f6-d6cd9d2353da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d65ad-53dd-4b25-ba46-843c9ee03030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1cfb44-af30-4de8-af69-1be4a1b3307d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
